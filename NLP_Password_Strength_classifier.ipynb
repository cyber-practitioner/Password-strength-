{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib as mt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import sklearn.metrics as skm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2810: expected 2 fields, saw 5\\nSkipping line 4641: expected 2 fields, saw 5\\nSkipping line 7171: expected 2 fields, saw 5\\nSkipping line 11220: expected 2 fields, saw 5\\nSkipping line 13809: expected 2 fields, saw 5\\nSkipping line 14132: expected 2 fields, saw 5\\nSkipping line 14293: expected 2 fields, saw 5\\nSkipping line 14865: expected 2 fields, saw 5\\nSkipping line 17419: expected 2 fields, saw 5\\nSkipping line 22801: expected 2 fields, saw 5\\nSkipping line 25001: expected 2 fields, saw 5\\nSkipping line 26603: expected 2 fields, saw 5\\nSkipping line 26742: expected 2 fields, saw 5\\nSkipping line 29702: expected 2 fields, saw 5\\nSkipping line 32767: expected 2 fields, saw 5\\nSkipping line 32878: expected 2 fields, saw 5\\nSkipping line 35643: expected 2 fields, saw 5\\nSkipping line 36550: expected 2 fields, saw 5\\nSkipping line 38732: expected 2 fields, saw 5\\nSkipping line 40567: expected 2 fields, saw 5\\nSkipping line 40576: expected 2 fields, saw 5\\nSkipping line 41864: expected 2 fields, saw 5\\nSkipping line 46861: expected 2 fields, saw 5\\nSkipping line 47939: expected 2 fields, saw 5\\nSkipping line 48628: expected 2 fields, saw 5\\nSkipping line 48908: expected 2 fields, saw 5\\nSkipping line 57582: expected 2 fields, saw 5\\nSkipping line 58782: expected 2 fields, saw 5\\nSkipping line 58984: expected 2 fields, saw 5\\nSkipping line 61518: expected 2 fields, saw 5\\nSkipping line 63451: expected 2 fields, saw 5\\nSkipping line 68141: expected 2 fields, saw 5\\nSkipping line 72083: expected 2 fields, saw 5\\nSkipping line 74027: expected 2 fields, saw 5\\nSkipping line 77811: expected 2 fields, saw 5\\nSkipping line 83958: expected 2 fields, saw 5\\nSkipping line 85295: expected 2 fields, saw 5\\nSkipping line 88665: expected 2 fields, saw 5\\nSkipping line 89198: expected 2 fields, saw 5\\nSkipping line 92499: expected 2 fields, saw 5\\nSkipping line 92751: expected 2 fields, saw 5\\nSkipping line 93689: expected 2 fields, saw 5\\nSkipping line 94776: expected 2 fields, saw 5\\nSkipping line 97334: expected 2 fields, saw 5\\nSkipping line 102316: expected 2 fields, saw 5\\nSkipping line 103421: expected 2 fields, saw 5\\nSkipping line 106872: expected 2 fields, saw 5\\nSkipping line 109363: expected 2 fields, saw 5\\nSkipping line 110117: expected 2 fields, saw 5\\nSkipping line 110465: expected 2 fields, saw 5\\nSkipping line 113843: expected 2 fields, saw 5\\nSkipping line 115634: expected 2 fields, saw 5\\nSkipping line 121518: expected 2 fields, saw 5\\nSkipping line 123692: expected 2 fields, saw 5\\nSkipping line 124708: expected 2 fields, saw 5\\nSkipping line 129608: expected 2 fields, saw 5\\nSkipping line 133176: expected 2 fields, saw 5\\nSkipping line 135532: expected 2 fields, saw 5\\nSkipping line 138042: expected 2 fields, saw 5\\nSkipping line 139485: expected 2 fields, saw 5\\nSkipping line 140401: expected 2 fields, saw 5\\nSkipping line 144093: expected 2 fields, saw 5\\nSkipping line 149850: expected 2 fields, saw 5\\nSkipping line 151831: expected 2 fields, saw 5\\nSkipping line 158014: expected 2 fields, saw 5\\nSkipping line 162047: expected 2 fields, saw 5\\nSkipping line 164515: expected 2 fields, saw 5\\nSkipping line 170313: expected 2 fields, saw 5\\nSkipping line 171325: expected 2 fields, saw 5\\nSkipping line 171424: expected 2 fields, saw 5\\nSkipping line 175920: expected 2 fields, saw 5\\nSkipping line 176210: expected 2 fields, saw 5\\nSkipping line 183603: expected 2 fields, saw 5\\nSkipping line 190264: expected 2 fields, saw 5\\nSkipping line 191683: expected 2 fields, saw 5\\nSkipping line 191988: expected 2 fields, saw 5\\nSkipping line 195450: expected 2 fields, saw 5\\nSkipping line 195754: expected 2 fields, saw 5\\nSkipping line 197124: expected 2 fields, saw 5\\nSkipping line 199263: expected 2 fields, saw 5\\nSkipping line 202603: expected 2 fields, saw 5\\nSkipping line 209960: expected 2 fields, saw 5\\nSkipping line 213218: expected 2 fields, saw 5\\nSkipping line 217060: expected 2 fields, saw 5\\nSkipping line 220121: expected 2 fields, saw 5\\nSkipping line 223518: expected 2 fields, saw 5\\nSkipping line 226293: expected 2 fields, saw 5\\nSkipping line 227035: expected 2 fields, saw 7\\nSkipping line 227341: expected 2 fields, saw 5\\nSkipping line 227808: expected 2 fields, saw 5\\nSkipping line 228516: expected 2 fields, saw 5\\nSkipping line 228733: expected 2 fields, saw 5\\nSkipping line 232043: expected 2 fields, saw 5\\nSkipping line 232426: expected 2 fields, saw 5\\nSkipping line 234490: expected 2 fields, saw 5\\nSkipping line 239626: expected 2 fields, saw 5\\nSkipping line 240461: expected 2 fields, saw 5\\nSkipping line 244518: expected 2 fields, saw 5\\nSkipping line 245395: expected 2 fields, saw 5\\nSkipping line 246168: expected 2 fields, saw 5\\nSkipping line 246655: expected 2 fields, saw 5\\nSkipping line 246752: expected 2 fields, saw 5\\nSkipping line 247189: expected 2 fields, saw 5\\nSkipping line 250276: expected 2 fields, saw 5\\nSkipping line 255327: expected 2 fields, saw 5\\nSkipping line 257094: expected 2 fields, saw 5\\n'\n",
      "b'Skipping line 264626: expected 2 fields, saw 5\\nSkipping line 265028: expected 2 fields, saw 5\\nSkipping line 269150: expected 2 fields, saw 5\\nSkipping line 271360: expected 2 fields, saw 5\\nSkipping line 273975: expected 2 fields, saw 5\\nSkipping line 274742: expected 2 fields, saw 5\\nSkipping line 276227: expected 2 fields, saw 5\\nSkipping line 279807: expected 2 fields, saw 5\\nSkipping line 283425: expected 2 fields, saw 5\\nSkipping line 287468: expected 2 fields, saw 5\\nSkipping line 292995: expected 2 fields, saw 5\\nSkipping line 293496: expected 2 fields, saw 5\\nSkipping line 293735: expected 2 fields, saw 5\\nSkipping line 295060: expected 2 fields, saw 5\\nSkipping line 296643: expected 2 fields, saw 5\\nSkipping line 296848: expected 2 fields, saw 5\\nSkipping line 308926: expected 2 fields, saw 5\\nSkipping line 310360: expected 2 fields, saw 5\\nSkipping line 317004: expected 2 fields, saw 5\\nSkipping line 318207: expected 2 fields, saw 5\\nSkipping line 331783: expected 2 fields, saw 5\\nSkipping line 333864: expected 2 fields, saw 5\\nSkipping line 335958: expected 2 fields, saw 5\\nSkipping line 336290: expected 2 fields, saw 5\\nSkipping line 343526: expected 2 fields, saw 5\\nSkipping line 343857: expected 2 fields, saw 5\\nSkipping line 344059: expected 2 fields, saw 5\\nSkipping line 348691: expected 2 fields, saw 5\\nSkipping line 353446: expected 2 fields, saw 5\\nSkipping line 357073: expected 2 fields, saw 5\\nSkipping line 359753: expected 2 fields, saw 5\\nSkipping line 359974: expected 2 fields, saw 5\\nSkipping line 366534: expected 2 fields, saw 5\\nSkipping line 369514: expected 2 fields, saw 5\\nSkipping line 377759: expected 2 fields, saw 5\\nSkipping line 379327: expected 2 fields, saw 5\\nSkipping line 380769: expected 2 fields, saw 5\\nSkipping line 381073: expected 2 fields, saw 5\\nSkipping line 381489: expected 2 fields, saw 5\\nSkipping line 386304: expected 2 fields, saw 5\\nSkipping line 387635: expected 2 fields, saw 5\\nSkipping line 389613: expected 2 fields, saw 5\\nSkipping line 392604: expected 2 fields, saw 5\\nSkipping line 393184: expected 2 fields, saw 5\\nSkipping line 395530: expected 2 fields, saw 5\\nSkipping line 396939: expected 2 fields, saw 5\\nSkipping line 397385: expected 2 fields, saw 5\\nSkipping line 397509: expected 2 fields, saw 5\\nSkipping line 402902: expected 2 fields, saw 5\\nSkipping line 405187: expected 2 fields, saw 5\\nSkipping line 408412: expected 2 fields, saw 5\\nSkipping line 419423: expected 2 fields, saw 5\\nSkipping line 420962: expected 2 fields, saw 5\\nSkipping line 425965: expected 2 fields, saw 5\\nSkipping line 427496: expected 2 fields, saw 5\\nSkipping line 438881: expected 2 fields, saw 5\\nSkipping line 439776: expected 2 fields, saw 5\\nSkipping line 440345: expected 2 fields, saw 5\\nSkipping line 445507: expected 2 fields, saw 5\\nSkipping line 445548: expected 2 fields, saw 5\\nSkipping line 447184: expected 2 fields, saw 5\\nSkipping line 448603: expected 2 fields, saw 5\\nSkipping line 451732: expected 2 fields, saw 5\\nSkipping line 458249: expected 2 fields, saw 5\\nSkipping line 460274: expected 2 fields, saw 5\\nSkipping line 467630: expected 2 fields, saw 5\\nSkipping line 473961: expected 2 fields, saw 5\\nSkipping line 476281: expected 2 fields, saw 5\\nSkipping line 478010: expected 2 fields, saw 5\\nSkipping line 478322: expected 2 fields, saw 5\\nSkipping line 479999: expected 2 fields, saw 5\\nSkipping line 480898: expected 2 fields, saw 5\\nSkipping line 481688: expected 2 fields, saw 5\\nSkipping line 485193: expected 2 fields, saw 5\\nSkipping line 485519: expected 2 fields, saw 5\\nSkipping line 486000: expected 2 fields, saw 5\\nSkipping line 489063: expected 2 fields, saw 5\\nSkipping line 494525: expected 2 fields, saw 5\\nSkipping line 495009: expected 2 fields, saw 5\\nSkipping line 501954: expected 2 fields, saw 5\\nSkipping line 508035: expected 2 fields, saw 5\\nSkipping line 508828: expected 2 fields, saw 5\\nSkipping line 509833: expected 2 fields, saw 5\\nSkipping line 510410: expected 2 fields, saw 5\\nSkipping line 518229: expected 2 fields, saw 5\\nSkipping line 520302: expected 2 fields, saw 5\\nSkipping line 520340: expected 2 fields, saw 5\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 525174: expected 2 fields, saw 5\\nSkipping line 526251: expected 2 fields, saw 5\\nSkipping line 529611: expected 2 fields, saw 5\\nSkipping line 531398: expected 2 fields, saw 5\\nSkipping line 534146: expected 2 fields, saw 5\\nSkipping line 544954: expected 2 fields, saw 5\\nSkipping line 553002: expected 2 fields, saw 5\\nSkipping line 553883: expected 2 fields, saw 5\\nSkipping line 553887: expected 2 fields, saw 5\\nSkipping line 553915: expected 2 fields, saw 5\\nSkipping line 554172: expected 2 fields, saw 5\\nSkipping line 563534: expected 2 fields, saw 5\\nSkipping line 565191: expected 2 fields, saw 5\\nSkipping line 574108: expected 2 fields, saw 5\\nSkipping line 574412: expected 2 fields, saw 5\\nSkipping line 575985: expected 2 fields, saw 5\\nSkipping line 580091: expected 2 fields, saw 5\\nSkipping line 582682: expected 2 fields, saw 5\\nSkipping line 585885: expected 2 fields, saw 5\\nSkipping line 590171: expected 2 fields, saw 5\\nSkipping line 591924: expected 2 fields, saw 5\\nSkipping line 592515: expected 2 fields, saw 5\\nSkipping line 593888: expected 2 fields, saw 5\\nSkipping line 596245: expected 2 fields, saw 5\\nSkipping line 607344: expected 2 fields, saw 5\\nSkipping line 607633: expected 2 fields, saw 5\\nSkipping line 610939: expected 2 fields, saw 5\\nSkipping line 613638: expected 2 fields, saw 5\\nSkipping line 615643: expected 2 fields, saw 5\\nSkipping line 615901: expected 2 fields, saw 5\\nSkipping line 617389: expected 2 fields, saw 5\\nSkipping line 634641: expected 2 fields, saw 5\\nSkipping line 635755: expected 2 fields, saw 5\\nSkipping line 646243: expected 2 fields, saw 5\\nSkipping line 647165: expected 2 fields, saw 5\\nSkipping line 648610: expected 2 fields, saw 5\\nSkipping line 648772: expected 2 fields, saw 5\\nSkipping line 651833: expected 2 fields, saw 5\\nSkipping line 653663: expected 2 fields, saw 5\\nSkipping line 656233: expected 2 fields, saw 5\\nSkipping line 656694: expected 2 fields, saw 5\\nSkipping line 659783: expected 2 fields, saw 5\\nSkipping line 660478: expected 2 fields, saw 5\\nSkipping line 661133: expected 2 fields, saw 5\\nSkipping line 661736: expected 2 fields, saw 5\\nSkipping line 669827: expected 2 fields, saw 5\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>kzde5577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>kino3434</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>visi7k1yr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>megzy123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>lamborghin1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      password  strength\n",
       "0     kzde5577         1\n",
       "1     kino3434         1\n",
       "2    visi7k1yr         1\n",
       "3     megzy123         1\n",
       "4  lamborghin1         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('NLP DATASET.csv',error_bad_lines=False)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['strength'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code to check all the missing values in my dataset (Data cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "password    1\n",
       "strength    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>367579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       password  strength\n",
       "367579      NaN         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['password'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "password    0\n",
       "strength    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x131a204a488>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['strength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "password_tuple=np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['kzde5577', 1],\n",
       "       ['kino3434', 1],\n",
       "       ['visi7k1yr', 1],\n",
       "       ...,\n",
       "       ['184520socram', 1],\n",
       "       ['marken22a', 1],\n",
       "       ['fxx4pw4g', 1]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "password_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffling randomly for robustness and to avoid data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(password_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[labels[0] for labels in password_tuple]\n",
    "y=[labels[1] for labels in password_tuple]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kzde5577',\n",
       " 'kzde5577',\n",
       " 'visi7k1yr',\n",
       " 'visi7k1yr',\n",
       " 'lamborghin1',\n",
       " 'visi7k1yr',\n",
       " 'kzde5577',\n",
       " 'v1118714',\n",
       " 'u6c8vhow',\n",
       " 'visi7k1yr',\n",
       " 'kino3434',\n",
       " 'kzde5577',\n",
       " 'lamborghin1',\n",
       " 'as326159',\n",
       " 'lamborghin1',\n",
       " 'u6c8vhow',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'kino3434',\n",
       " 'jerusalem393',\n",
       " 'as326159',\n",
       " '52558000aaa',\n",
       " 'visi7k1yr',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'asv5o9yu',\n",
       " 'klara-tershina3H',\n",
       " '6975038lp',\n",
       " 'jerusalem393',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " 'czuodhj972',\n",
       " 'jerusalem393',\n",
       " 'as326159',\n",
       " 'visi7k1yr',\n",
       " 'asv5o9yu',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'prisonbreak1',\n",
       " 'schalke04',\n",
       " 'klara-tershina3H',\n",
       " 'fk9qi21m',\n",
       " 'schalke04',\n",
       " 'kino3434',\n",
       " 'universe2908',\n",
       " 'prisonbreak1',\n",
       " 'intel1',\n",
       " 'kswa2mrv',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " '612035180tok',\n",
       " 'faranumar91',\n",
       " 'fahad123',\n",
       " 'cigicigi123',\n",
       " 'juliel009',\n",
       " '0169395484a',\n",
       " 'prisonbreak1',\n",
       " '6975038lp',\n",
       " 'kino3434',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'lsdlsd1',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " 'g067057895',\n",
       " 'juliel009',\n",
       " 'c3h8bkzr',\n",
       " 'gaymaids1',\n",
       " 'kswa2mrv',\n",
       " 'yk530mg8',\n",
       " 'khmer100.03278&?><Mnb',\n",
       " 'ejeko677',\n",
       " 'universe2908',\n",
       " 'kswa2mrv',\n",
       " 'memjan123',\n",
       " 'tamanagung6',\n",
       " 'rntprns7',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'patri1973',\n",
       " 'obstacle25',\n",
       " 'juliel009',\n",
       " 'go7kew7a2po',\n",
       " 'a2531106',\n",
       " 'lsdlsd1',\n",
       " 'v1118714',\n",
       " '283671gus',\n",
       " 'g067057895',\n",
       " 'fahad123',\n",
       " 'hpqkoxsn5',\n",
       " 'juliana19',\n",
       " 'gill02',\n",
       " 'yqugu927',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'as326159',\n",
       " 'lsdlsd1',\n",
       " 'kjkjkj1',\n",
       " 'gaymaids1',\n",
       " 'fk9qi21m',\n",
       " 'gill02',\n",
       " '0169395484a',\n",
       " 'kino3434',\n",
       " 'mickael12',\n",
       " 'fk9qi21m',\n",
       " 'sbl571017',\n",
       " 'khmer100.03278&?><Mnb',\n",
       " 'cesarmaio1',\n",
       " 'p2share',\n",
       " 'hayhayq2',\n",
       " 'ejeko677',\n",
       " 'prisonbreak1',\n",
       " 'kjkjkj1',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'jalal123456',\n",
       " 'RqsuUsDYxNgr8T40',\n",
       " 'c3h8bkzr',\n",
       " 'snolyuj04',\n",
       " '746xitEGiqObog',\n",
       " 'elyass15@ajilent-ci',\n",
       " '2021848709.',\n",
       " 'j09000',\n",
       " 'jalal123456',\n",
       " 'fahad123',\n",
       " 'TyWM72UNEex8Q8Y',\n",
       " '64959rodro',\n",
       " 'g067057895',\n",
       " 'lsdlsd1',\n",
       " 'a2531106',\n",
       " 'TyWM72UNEex8Q8Y',\n",
       " 'bgrvl80',\n",
       " '838188linh',\n",
       " 'poseidon2011',\n",
       " 'bozoxik602',\n",
       " 'p2share',\n",
       " 'znbl5tj1',\n",
       " 'yk530mg8',\n",
       " 'czuodhj972',\n",
       " 'patri1973',\n",
       " 'asgaliu11',\n",
       " 'RqsuUsDYxNgr8T40',\n",
       " 'khmer100.03278&?><Mnb',\n",
       " '2akira2',\n",
       " '123477889a',\n",
       " '746xitEGiqObog',\n",
       " 'exitos2009',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'calcifer32',\n",
       " 'jonothepoop1',\n",
       " 'khmer100.03278&?><Mnb',\n",
       " 'k9b8cz6aj2',\n",
       " '283671gus',\n",
       " 'z3ro1sm',\n",
       " 'memjan123',\n",
       " 'yk530mg8',\n",
       " 'ikanez886',\n",
       " 'jerusalem393',\n",
       " 'g067057895',\n",
       " 'juliana19',\n",
       " 'as326159',\n",
       " 'kinga22',\n",
       " '147963asd',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " '612035180tok',\n",
       " 's9830950044',\n",
       " 'openup12',\n",
       " 'kjkjkj1',\n",
       " 'elonex24',\n",
       " 'czuodhj972',\n",
       " 'rogyh820',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'alimagik1',\n",
       " 's9830950044',\n",
       " 'yitbos77',\n",
       " 'yu4cmn',\n",
       " 'prisonbreak1',\n",
       " 'kino3434',\n",
       " 'k9b8cz6aj2',\n",
       " 'vehat387',\n",
       " 'p2share',\n",
       " 'omakiva153',\n",
       " 'intel1',\n",
       " 'kunyukbabi69',\n",
       " '6tequila6',\n",
       " 'znbl5tj1',\n",
       " 'yuri110995',\n",
       " 'poseidon2011',\n",
       " 'gkrqjs6',\n",
       " '147963asd',\n",
       " 'd04m11',\n",
       " 'x8512514',\n",
       " 'alchimie79',\n",
       " 'wisal1234',\n",
       " 'sanki1',\n",
       " 'RPFUOUDQwMwVW0AS',\n",
       " 'go7kew7a2po',\n",
       " 'polo2014',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'aquhih220',\n",
       " 'marita1',\n",
       " 'lsdlsd1',\n",
       " 'ass359',\n",
       " 'meopvywk628',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " 'kjkjkj1',\n",
       " '0169395484a',\n",
       " '0169395484a',\n",
       " 'mickael12',\n",
       " 'intel1',\n",
       " 'moken7',\n",
       " 'v10rica',\n",
       " 'faranumar91',\n",
       " 'ok>bdk',\n",
       " 'afs34214',\n",
       " 'peluchin4',\n",
       " 'aquhih220',\n",
       " 'megzy123',\n",
       " '5gzj5uf',\n",
       " 'intel1',\n",
       " 'sbl571017',\n",
       " 'z7zbgIDkzMQeHUd9',\n",
       " 'juliana19',\n",
       " 'kunyukbabi69',\n",
       " 'farrukhcse12',\n",
       " 'RqsuUsDYxNgr8T40',\n",
       " '123477889a',\n",
       " '159951josh',\n",
       " 'ubojig109',\n",
       " 'AVYq1lDE4MgAZfNt',\n",
       " 'ebacuro434',\n",
       " 'gozv3e5',\n",
       " 'obstacle25',\n",
       " 'jalingo1',\n",
       " 'vgnfs495vp',\n",
       " 'samael666',\n",
       " 'rntprns7',\n",
       " 'prisonbreak1',\n",
       " 'kikeq102',\n",
       " 'metopelo1623',\n",
       " 'nello11',\n",
       " 'mohantra1',\n",
       " 'teste10',\n",
       " 'patri1973',\n",
       " 'olmaz.',\n",
       " '2fakjv',\n",
       " 'oekojWyH120063',\n",
       " 'www32223222',\n",
       " 'juliel009',\n",
       " '283671gus',\n",
       " '12345yolanda',\n",
       " 'atigi839',\n",
       " 'bugatti01',\n",
       " 'znbl5tj1',\n",
       " '2021848709.',\n",
       " 'p2share',\n",
       " '2021848709.',\n",
       " 'vehat387',\n",
       " 'khmer100.03278&?><Mnb',\n",
       " 'il0vey0u',\n",
       " 'p@sslng2diword',\n",
       " 'X9WVojjE4MgVAIiR',\n",
       " 'holamundo1',\n",
       " 'lsdlsd1',\n",
       " 'cesarmaio1',\n",
       " 'a0972986650',\n",
       " 'jonothepoop1',\n",
       " 'kswa2mrv',\n",
       " 'rogyh820',\n",
       " 'teemteem97',\n",
       " '159951josh',\n",
       " 'j09000',\n",
       " 'XqMB7vDMzOQocAFV',\n",
       " 'yqugu927',\n",
       " 'rLLh4WDQ2OAWbDO5',\n",
       " 'ejeko677',\n",
       " 'kayal123',\n",
       " 'uxyloga692',\n",
       " 'uou2dae',\n",
       " 'owote852',\n",
       " 'witek1709',\n",
       " 'kinga22',\n",
       " '6975038lp',\n",
       " '7mV0pKTA3MgHy8Jv',\n",
       " 'XqMB7vDMzOQocAFV',\n",
       " 'lsdlsd1',\n",
       " 'atigi839',\n",
       " 'gill02',\n",
       " '6yy6yy',\n",
       " '7942vikas',\n",
       " 'Iamthelegend1!',\n",
       " 'tin030201',\n",
       " '12345yolanda',\n",
       " 'b4NbTxDEyNgG141J',\n",
       " 'fahad123',\n",
       " 'xW8-3w7-MFB-CKH',\n",
       " 'sarahi1628',\n",
       " 'ass359',\n",
       " 'ubojig109',\n",
       " 'alchimie79',\n",
       " 'AS0130066',\n",
       " 'k9b8cz6aj2',\n",
       " 'mathilde54550',\n",
       " 'poseidon2011',\n",
       " 'czuodhj972',\n",
       " 'mickael12',\n",
       " 'ajyrew547',\n",
       " 'abizar08',\n",
       " 'woon12',\n",
       " 'wisal1234',\n",
       " 'xanyrum650',\n",
       " 'snolyuj04',\n",
       " 'lamborghin1',\n",
       " 'sydney213',\n",
       " 'xiau5ff',\n",
       " 'e667794c1d',\n",
       " 'edcmki90',\n",
       " '123477889a',\n",
       " 'gandhi8513',\n",
       " 'llahetihw1',\n",
       " 'ass359',\n",
       " 'barboza221294',\n",
       " '7mV0pKTA3MgHy8Jv',\n",
       " 'openup12',\n",
       " 'wycinu436',\n",
       " 'kunyukbabi69',\n",
       " 'ikanez886',\n",
       " 'warriors08',\n",
       " 'josue12',\n",
       " '1234159hero',\n",
       " '6tequila6',\n",
       " '3vszncp4',\n",
       " 's4m2dx9e6',\n",
       " '26522876p',\n",
       " 'cdann123',\n",
       " 'portales1',\n",
       " '0169395484a',\n",
       " 'juliana19',\n",
       " '20010509wang',\n",
       " '215466kenyi',\n",
       " 'uou2dae',\n",
       " 'hqh2eYjQxOQPYIsA',\n",
       " 'uou2dae',\n",
       " '631ihOZogELoVap',\n",
       " '2021848709.',\n",
       " 'kry1z9',\n",
       " 'gkrqjs6',\n",
       " 'bugatti01',\n",
       " 'koabcswzt3',\n",
       " 'nicolas05',\n",
       " 'abizar08',\n",
       " 'universe2908',\n",
       " 'asgaliu11',\n",
       " '3y6iwef2g6',\n",
       " 'taurofive16',\n",
       " 'isqizkg1',\n",
       " 'TyWM72UNEex8Q8Y',\n",
       " 'jalingo1',\n",
       " 'warriors08',\n",
       " '929865yt',\n",
       " 'mustang337',\n",
       " 'juliana19',\n",
       " '7942vikas',\n",
       " 'jerusalem393',\n",
       " 'q0pv0fk',\n",
       " 'cockw0mble',\n",
       " 'examy624',\n",
       " 'kuntz80',\n",
       " 'fk9qi21m',\n",
       " 'gdfn76',\n",
       " 'wibi182d',\n",
       " 'mario489800',\n",
       " 'sbl571017',\n",
       " 'rLLh4WDQ2OAWbDO5',\n",
       " 'examy624',\n",
       " '6yy6yy',\n",
       " '1234159hero',\n",
       " 'tahseen75',\n",
       " 'yuri110995',\n",
       " 'peluchin4',\n",
       " 'hasan18',\n",
       " '10Erjrlmebup0n',\n",
       " 'lrhxmevb620',\n",
       " '6yy6yy',\n",
       " 'sofietou74',\n",
       " 'koabcswzt3',\n",
       " 'idofo673',\n",
       " 'oatcake87',\n",
       " '123nicole',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " '147963asd',\n",
       " 'nLIGyhTU1NQTAp6u',\n",
       " 'matiz4533',\n",
       " 'beijing168',\n",
       " 'krishna2',\n",
       " 'x8512514',\n",
       " 'llahetihw1',\n",
       " 's9830950044',\n",
       " 'v10rica',\n",
       " 'jalal123456',\n",
       " 'Iamthelegend1!',\n",
       " 'kP82iqDMxNgBMxBP',\n",
       " 'sbaUsoTA1OAzuevI',\n",
       " 'mazdarx7',\n",
       " '123maxbala',\n",
       " 'icap12',\n",
       " 'xlxlxl777',\n",
       " 'ebacuro434',\n",
       " 'khurram_',\n",
       " '6yy6yy',\n",
       " 'z888888',\n",
       " 'yu4cmn',\n",
       " '631ihOZogELoVap',\n",
       " 'nicolas05',\n",
       " 'hayhayq2',\n",
       " 'openup12',\n",
       " 'jUV4dSDQwNwPpA36',\n",
       " 'demon10',\n",
       " 'elabadmin1386',\n",
       " '2GnTStTE4Mw4MTwv',\n",
       " 'j09000',\n",
       " 'mathilde54550',\n",
       " 'sd6x9s3s',\n",
       " 'edcmki90',\n",
       " '1katertje',\n",
       " 'kikeq102',\n",
       " 'beijing168',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " 'hasan18',\n",
       " 'llahetihw1',\n",
       " '2yz4ewwg',\n",
       " 'nicolas05',\n",
       " 'oscar69',\n",
       " 'spl51190595',\n",
       " 'aquhih220',\n",
       " 'klara-tershina3H',\n",
       " '4fqa52vecr',\n",
       " 'd6VyrkFV6oblxNs5N8cW',\n",
       " 'folashade1',\n",
       " '64whbrb351',\n",
       " 'kVczcljg4OA25Aeb',\n",
       " 'may112001',\n",
       " 'llahetihw1',\n",
       " 'parvizrus13',\n",
       " 'owote852',\n",
       " 'k1k2k3k4k5k6',\n",
       " 'calcifer32',\n",
       " '3y6iwef2g6',\n",
       " 'yut0838828185',\n",
       " 'universe2908',\n",
       " 'marita1',\n",
       " 'jalal123456',\n",
       " 'terrassa6',\n",
       " '64whbrb351',\n",
       " 'graciela2',\n",
       " 'www32223222',\n",
       " 'k9b8cz6aj2',\n",
       " 'graciela2',\n",
       " 'kswa2mrv',\n",
       " 'bgrvl80',\n",
       " 'kinga22',\n",
       " 'a2531106',\n",
       " 'warriors08',\n",
       " 'wycinu436',\n",
       " 'fk9qi21m',\n",
       " 'UF1Z2WjE5Mg26R1K',\n",
       " 'gkrqjs6',\n",
       " 'e667794c1d',\n",
       " 'QWERTY0011',\n",
       " 'a2531106',\n",
       " 'graciela2',\n",
       " 'portales1',\n",
       " 'password0880',\n",
       " 'moken7',\n",
       " 'pekai2004',\n",
       " 'graciela2',\n",
       " 'faranumar91',\n",
       " 'witek1709',\n",
       " 'jonothepoop1',\n",
       " 'han19660120',\n",
       " 'ass359',\n",
       " 'exitos2009',\n",
       " 'kyodai666',\n",
       " 'polo2014',\n",
       " 'asdasdf1',\n",
       " 'owote852',\n",
       " '20010509wang',\n",
       " 'e667794c1d',\n",
       " 'kyxvufl37',\n",
       " 'khaled12',\n",
       " 'a2531106',\n",
       " 'oatcake87',\n",
       " 'megzy123',\n",
       " 'hasan18',\n",
       " 'sanki1',\n",
       " 'puegwajy416',\n",
       " 'lymuvop730',\n",
       " 'kyxvufl37',\n",
       " 'pikey231',\n",
       " 'uziwocy148',\n",
       " 'taurofive16',\n",
       " 'kjkjkj1',\n",
       " 'memjan123',\n",
       " '3y6iwef2g6',\n",
       " 'yk530mg8',\n",
       " '2GnTStTE4Mw4MTwv',\n",
       " '07dpv1127b',\n",
       " 'kinga22',\n",
       " 'autan88',\n",
       " 'VMjz4eTkxNAbOyUU',\n",
       " '4lgYVfzk1MwuzHcn',\n",
       " 'il0vey0u',\n",
       " 'sydney213',\n",
       " 'may112001',\n",
       " 'pikey231',\n",
       " 'Ju6BIMTU0MwYXtL4',\n",
       " 'barra23',\n",
       " '4lgYVfzk1MwuzHcn',\n",
       " 'colorado27',\n",
       " '4TXr5KDYxNQVTo4g',\n",
       " '2akira2',\n",
       " 'calcifer32',\n",
       " 'beijing168',\n",
       " 'aslpls2009',\n",
       " 'pilatyj280',\n",
       " 'idofo673',\n",
       " 'kevin24',\n",
       " 'q0pv0fk',\n",
       " 'faranumar91',\n",
       " 'kikeq102',\n",
       " 'memjan123',\n",
       " 'il0vey0u',\n",
       " 'saule123',\n",
       " 'cristiano7',\n",
       " 'v1118714',\n",
       " 'IjUcOtYqAwel725',\n",
       " 'weicat12',\n",
       " 'gvczfel801',\n",
       " 'n501iomf',\n",
       " '612035180tok',\n",
       " 'TyWM72UNEex8Q8Y',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " '20010509wang',\n",
       " 'woogee04',\n",
       " '52558000aaa',\n",
       " 'u6c8vhow',\n",
       " 'a110804032',\n",
       " '159951josh',\n",
       " 'exitos2009',\n",
       " 'zidadoh258',\n",
       " 's4m2dx9e6',\n",
       " 'obstacle25',\n",
       " 'fnmsdha476',\n",
       " 'seller1',\n",
       " 'c3h8bkzr',\n",
       " 'bang6k',\n",
       " 'd6VyrkFV6oblxNs5N8cW',\n",
       " 'damyvo114',\n",
       " 'seng987321',\n",
       " 'seller1',\n",
       " 'den019520',\n",
       " 'polo2014',\n",
       " 'aosmaxd0',\n",
       " 'wearehis7',\n",
       " '33kanun03',\n",
       " 'may112001',\n",
       " 'jalal123456',\n",
       " '4lgYVfzk1MwuzHcn',\n",
       " 'caramelo9',\n",
       " 'Iamthelegend1!',\n",
       " 'potatobus150',\n",
       " 'eVl19ADIxNAmU09N',\n",
       " 'trabajonet9',\n",
       " 'tin030201',\n",
       " '1justogax',\n",
       " 'viri13',\n",
       " 'bugatti01',\n",
       " 'taurofive16',\n",
       " 'idofo673',\n",
       " 'mathilde54550',\n",
       " 'WUt9IZzE0OQ7PkNE',\n",
       " 'ixehawojEPe418',\n",
       " 'tomas7896',\n",
       " 'juliana19',\n",
       " 'mario489800',\n",
       " 'yu4cmn',\n",
       " 'den019520',\n",
       " 'tin030201',\n",
       " 'limichan99',\n",
       " '0VKWoODkwOAc0pZK',\n",
       " 'examy624',\n",
       " 'e667794c1d',\n",
       " 'sbl571017',\n",
       " 'kitty555',\n",
       " 'hqh2eYjQxOQPYIsA',\n",
       " '64whbrb351',\n",
       " 'cristiano7',\n",
       " 'k9b8cz6aj2',\n",
       " 'planes123',\n",
       " 'patty94',\n",
       " 'jr88072635',\n",
       " 'polo2014',\n",
       " 'cerner09',\n",
       " 'deryxi704',\n",
       " 'autan88',\n",
       " '248sUqiFEJuRag',\n",
       " 'macias2010',\n",
       " 'bugatti01',\n",
       " 'yjuqseb416',\n",
       " 'jUV4dSDQwNwPpA36',\n",
       " 'ass359',\n",
       " 'hola45',\n",
       " 'gerardway1',\n",
       " 'idofo673',\n",
       " 'poilkjmnb987',\n",
       " 'kunyukbabi69',\n",
       " 'wearehis7',\n",
       " 'kino3434',\n",
       " 'bencike7',\n",
       " 'buqodym199',\n",
       " 'b9m7cxcgc',\n",
       " 'desmondkok21',\n",
       " 'plumilla1',\n",
       " 'kzde5577',\n",
       " 'gill02',\n",
       " 'sbnivetha123',\n",
       " 'ga98SIzk0NwhiZaE',\n",
       " 'qwekl12',\n",
       " 'parvizrus13',\n",
       " 'xanyrum650',\n",
       " 'www32223222',\n",
       " 'zgmfnwuq25',\n",
       " 'yk530mg8',\n",
       " 'b9m7cxcgc',\n",
       " 'ykfums1',\n",
       " 'jerusalem393',\n",
       " 'terrassa6',\n",
       " 'pekai2004',\n",
       " '123net123',\n",
       " 'XqMB7vDMzOQocAFV',\n",
       " 'aosmaxd0',\n",
       " 'tucagu356',\n",
       " '159951josh',\n",
       " '3y6iwef2g6',\n",
       " 'metopelo1623',\n",
       " 'shotiko18',\n",
       " 'atigi839',\n",
       " 'b9m7cxcgc',\n",
       " 'taulant123',\n",
       " 'taulant123',\n",
       " 'marita1',\n",
       " 'z7zbgIDkzMQeHUd9',\n",
       " 'osimeytju12',\n",
       " 'v1s2c3456',\n",
       " 'gaymaids1',\n",
       " 'finisterra1',\n",
       " 'junaid5',\n",
       " 'seng987321',\n",
       " 'sydney213',\n",
       " 'keithar1',\n",
       " 'ryjypes139',\n",
       " 'pekai2004',\n",
       " 'oatcake87',\n",
       " 'demon10',\n",
       " 'moimoimoi9',\n",
       " 'vardhan19',\n",
       " 'kjkjkj1',\n",
       " '2652033abc',\n",
       " 'xiau5ff',\n",
       " 'rsuvxz08b',\n",
       " 'jbtcnd6',\n",
       " 'eth36498',\n",
       " 'examy624',\n",
       " 'fahad123',\n",
       " 'purpledog1992',\n",
       " 'cyborged69',\n",
       " 'w9209640',\n",
       " '23deagosto',\n",
       " 'xp;ysmybst',\n",
       " 'tim80327',\n",
       " 'b98nwtpriyesh',\n",
       " 'jesmond26',\n",
       " 'viri13',\n",
       " 'kzde5577',\n",
       " 'teste10',\n",
       " 'skoda06',\n",
       " 'uoaef06gfqeb',\n",
       " 'natalia12',\n",
       " 'cigicigi123',\n",
       " 'naseKoBUMIg295',\n",
       " 'gpc151192',\n",
       " '2021848709.',\n",
       " '2d0d7qfz',\n",
       " '3f5xd41l0ik7',\n",
       " 'sasuke4',\n",
       " 'anon13',\n",
       " 'kjkjkj1',\n",
       " '16731673ir',\n",
       " '847XagYxUHUXOW',\n",
       " 'lqksuym982',\n",
       " 'k1k2k3k4k5k6',\n",
       " 'sanjaime1',\n",
       " 'mike09',\n",
       " '847XagYxUHUXOW',\n",
       " 'openup12',\n",
       " 'sergius1964',\n",
       " 'olmaz.',\n",
       " 'lzhzad1989',\n",
       " '1jancok',\n",
       " 'asv5o9yu',\n",
       " 'just1n0k',\n",
       " 'x8512514',\n",
       " 'sd6x9s3s',\n",
       " 'seng987321',\n",
       " 'pekai2004',\n",
       " 'jeeves123',\n",
       " '1qa2ws3ed4rf',\n",
       " 'matiz4533',\n",
       " 'pastorius88',\n",
       " 'gvczfel801',\n",
       " 'znbl5tj1',\n",
       " '1234159hero',\n",
       " 'obstacle25',\n",
       " 'weicat12',\n",
       " 'icap12',\n",
       " 'ejeko677',\n",
       " 'afavin964',\n",
       " 'h1h2h3h4h5',\n",
       " 'olyucskw52',\n",
       " 'nK0yKXTU0NQHZE2e',\n",
       " 'yut0838828185',\n",
       " 'vietnga92',\n",
       " '3y6iwef2g6',\n",
       " 'sanjaime1',\n",
       " 'x57669',\n",
       " '16731673ir',\n",
       " 'Ju6BIMTU0MwYXtL4',\n",
       " 'change201',\n",
       " 'DTUQG5jU5MwmR1L9',\n",
       " 'edcmki90',\n",
       " 'utuham322',\n",
       " 'utuham322',\n",
       " 'kenyu001',\n",
       " 'warriors08',\n",
       " 'ajyrew547',\n",
       " 'xzeyfbi495',\n",
       " 'ayles2266',\n",
       " 'pato221182',\n",
       " 'aio42fv',\n",
       " 'walterivl13',\n",
       " 't8IkFRDIxMAFV2JW',\n",
       " '1qa2ws3ed4rf',\n",
       " 'macias2010',\n",
       " '838188linh',\n",
       " '838188linh',\n",
       " 'ajyrew547',\n",
       " 'coy29061994',\n",
       " 'hosna1368',\n",
       " 'hpqkoxsn5',\n",
       " 'falre1524',\n",
       " 'eth36498',\n",
       " 'raykuaz32',\n",
       " 'woaini0',\n",
       " 'omakiva153',\n",
       " '929865yt',\n",
       " 'xyws951753',\n",
       " 'v1s2c3456',\n",
       " 'gracimir87',\n",
       " 'jalingo1',\n",
       " 'icap12',\n",
       " '0169395484a',\n",
       " '101010hadis',\n",
       " 'webhostv1t1n',\n",
       " 'natalia12',\n",
       " 'pato221182',\n",
       " 'avanakit72',\n",
       " 'nikolas369',\n",
       " 'aqyba894',\n",
       " 'uzifyc502',\n",
       " 'paladinas1',\n",
       " 'lamborghin1',\n",
       " '0870330135a',\n",
       " 'ass359',\n",
       " 'onurb1994',\n",
       " 'hello2104',\n",
       " 'IP1yaTDUzOQWAeI5',\n",
       " 'znbl5tj1',\n",
       " '4lgYVfzk1MwuzHcn',\n",
       " 'IP1yaTDUzOQWAeI5',\n",
       " 'aziz098765',\n",
       " 'prisonbreak1',\n",
       " 'asgaliu11',\n",
       " 'megdam55',\n",
       " 'seeyouagain1',\n",
       " 'jbtcnd6',\n",
       " 'b9m7cxcgc',\n",
       " 's0xwym7h',\n",
       " 'in595462',\n",
       " '6yy6yy',\n",
       " '631ihOZogELoVap',\n",
       " 'purpledog1992',\n",
       " 'taulant123',\n",
       " 'wo9aiwangyan',\n",
       " 'numero2',\n",
       " 'Jovan13lovekenthjusvan4ever',\n",
       " 'enziitoo1234',\n",
       " 'moimoimoi9',\n",
       " 'gill02',\n",
       " 'yv3hpf',\n",
       " 'Herzberg@ABBOTT33656888commerce',\n",
       " '215466kenyi',\n",
       " 'deryxi704',\n",
       " 'krumbul123',\n",
       " 'yu86640132',\n",
       " 'bugatti01',\n",
       " 'killer5',\n",
       " 'pastorius88',\n",
       " '23deagosto',\n",
       " 'azizi120583',\n",
       " 'RJXDk1zEyMwpyq0U',\n",
       " 'megzy123',\n",
       " 'xanyrum650',\n",
       " 'skoda06',\n",
       " 'acgyj188',\n",
       " 'karl88',\n",
       " 'pedronha96',\n",
       " 'jalal123456',\n",
       " 'RPFUOUDQwMwVW0AS',\n",
       " 'jytifok873',\n",
       " 'uxyloga692',\n",
       " 'beijing168',\n",
       " 'bang6k',\n",
       " '3CgRg8DA1NQY1iEj',\n",
       " 'teste10',\n",
       " 'gonzalez1559',\n",
       " 'Zdyf0kjMzNQycqPx',\n",
       " 'pacific52',\n",
       " 'xawipy995',\n",
       " 'xlxlxl777',\n",
       " 'gjm666',\n",
       " 'g067057895',\n",
       " 'kayal123',\n",
       " 'm4r4hne',\n",
       " 'ajyrew547',\n",
       " 'jerusalem393',\n",
       " 'zoblin80',\n",
       " '3vszncp4',\n",
       " 'willboss13',\n",
       " 'limichan99',\n",
       " 'groster152',\n",
       " 's0xwym7h',\n",
       " 'z7zbgIDkzMQeHUd9',\n",
       " 'patata91',\n",
       " 'moken7',\n",
       " 'jgsxkcp639',\n",
       " 'lsdlsd1',\n",
       " 'Oshity07142014',\n",
       " 'kyodai666',\n",
       " 'iacool99',\n",
       " 'purpledog1992',\n",
       " 'yuri110995',\n",
       " '12345yolanda',\n",
       " 'ziKYRyqewaxE717',\n",
       " 'mdaffandi74',\n",
       " 'arum210684',\n",
       " 'twil8x0',\n",
       " 'yut0838828185',\n",
       " '1597535youssi',\n",
       " 'rqmswof2llb0',\n",
       " 'satelite31',\n",
       " 'midgeman8505',\n",
       " 'teemteem97',\n",
       " '1234159hero',\n",
       " 'alimagik1',\n",
       " '64959rodro',\n",
       " 'w9209640',\n",
       " '215466kenyi',\n",
       " 'growerz543',\n",
       " 'ashaii12',\n",
       " 'zeeshanbhai1',\n",
       " 'portales1',\n",
       " 'control9',\n",
       " 'as8594505',\n",
       " 'jbtcnd6',\n",
       " 'schalke04',\n",
       " 'qefoquf1uf',\n",
       " '283671gus',\n",
       " '94311163nobp',\n",
       " 'e667794c1d',\n",
       " 'ebogel225',\n",
       " 'intel1',\n",
       " 'aio42fv',\n",
       " 'yu86640132',\n",
       " 'regodib479',\n",
       " '000martin',\n",
       " 'zjl0kx03',\n",
       " 'yqugu927',\n",
       " 'jbiz04h4',\n",
       " 'calcifer32',\n",
       " '7mV0pKTA3MgHy8Jv',\n",
       " 'mel008',\n",
       " 'numero2',\n",
       " 'witek1709',\n",
       " 'sanjaime1',\n",
       " 'in595462',\n",
       " 'kuntz80',\n",
       " 'ass359',\n",
       " 'arum210684',\n",
       " 'mialr325',\n",
       " 'warriors08',\n",
       " 'overlord3127',\n",
       " 'examy624',\n",
       " 'juany57',\n",
       " 'p3rf3ct0',\n",
       " 'ajyrew547',\n",
       " 'Ju6BIMTU0MwYXtL4',\n",
       " 'paladinas1',\n",
       " 'UF1Z2WjE5Mg26R1K',\n",
       " 'rrilni1747',\n",
       " '4osxw4r',\n",
       " '2yz4ewwg',\n",
       " 'hodaq103',\n",
       " 'YanH2kzgxMw1AsXA',\n",
       " 'hylefup708',\n",
       " 'examy624',\n",
       " 'mtvwyz001',\n",
       " 'paulino123',\n",
       " 'bugatti01',\n",
       " 'juliel009',\n",
       " 'poilkjmnb987',\n",
       " 'caramelo9',\n",
       " 'aquhih220',\n",
       " 'poseidon2011',\n",
       " 'kunyukbabi69',\n",
       " 'a1233210',\n",
       " 'idofo673',\n",
       " 'ihana906',\n",
       " 'sbl571017',\n",
       " 'jorge1489',\n",
       " '123net123',\n",
       " 'ayles2266',\n",
       " 'mxvurqyws5',\n",
       " 'evivad588',\n",
       " 'cdann123',\n",
       " '00J46LTAxMgME7C4',\n",
       " 'paladinas1',\n",
       " 'enziitoo1234',\n",
       " 'clave08',\n",
       " 'kdl9cl53',\n",
       " 'jeeves123',\n",
       " 'BZVQZBTM1MApRV7s',\n",
       " 'owary200',\n",
       " 'arigato3',\n",
       " 'ubojig109',\n",
       " 'Zdyf0kjMzNQycqPx',\n",
       " 'qefoquf1uf',\n",
       " 'jcav26',\n",
       " 'natalia12',\n",
       " 'pablo321159',\n",
       " 'v1118714',\n",
       " 'y75oak7b',\n",
       " 'synyxyr723',\n",
       " 'jonothepoop1',\n",
       " 'satelite31',\n",
       " 'mtvwyz001',\n",
       " '1satguru1',\n",
       " 'tot2531s',\n",
       " '123477889a',\n",
       " 'enziitoo1234',\n",
       " '6U3xJzDE3Nggz3L1',\n",
       " 'mtvwyz001',\n",
       " 'bghuyku37',\n",
       " 'tukaxo486',\n",
       " 'kate13',\n",
       " 'Staterkom20081993',\n",
       " 'terrassa6',\n",
       " '2021848709.',\n",
       " 'v1s2c3456',\n",
       " 'mussuh4',\n",
       " 'hard7ware',\n",
       " 'bb9530',\n",
       " '72o0yzekib4',\n",
       " 'd4xQ3LjUwMQFVCYQ',\n",
       " 'sd6x9s3s',\n",
       " '147963asd',\n",
       " 'goony01',\n",
       " 'x8512514',\n",
       " 's9830950044',\n",
       " 'uvazyd135',\n",
       " 'pilatyj280',\n",
       " 'sd6x9s3s',\n",
       " 'julie1989',\n",
       " 'tspirits08',\n",
       " '1jancok',\n",
       " '0VKWoODkwOAc0pZK',\n",
       " 'sknq7m0',\n",
       " 'lofebop480',\n",
       " 'taogamma21',\n",
       " 'woon12',\n",
       " 'pHyqueDIyNQ8vmhb',\n",
       " '2010server',\n",
       " 'ydkmujrze3',\n",
       " 'orejuby808',\n",
       " 'qwe000',\n",
       " 'taurofive16',\n",
       " 'gerylyx688',\n",
       " '2863e00016',\n",
       " 'pr0f1s10',\n",
       " 'designer1206',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a custom function to split input into characters of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_divide_char(inputs):\n",
    "    character=[]\n",
    "    for i in inputs:\n",
    "        character.append(i)\n",
    "    return character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'z', 'd', 'e', '5', '5', '7', '7']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_divide_char('kzde5577')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import TF-IDF vectorizer to convert String data into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(tokenizer=word_divide_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply TF-IDF vectorizer on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x04',\n",
       " '\\x05',\n",
       " '\\x06',\n",
       " '\\x08',\n",
       " '\\x0e',\n",
       " '\\x0f',\n",
       " '\\x10',\n",
       " '\\x11',\n",
       " '\\x16',\n",
       " '\\x17',\n",
       " '\\x19',\n",
       " '\\x1b',\n",
       " '\\x1c',\n",
       " '\\x1e',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '\\x7f',\n",
       " '\\x81',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x129 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_document_vector=X[0]\n",
    "first_document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.56701387],\n",
       "        [0.        ],\n",
       "        [0.59142939],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.28581409],\n",
       "        [0.22142738],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.2922998 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.33548115],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_document_vector.T.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.5914293898242222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.5670138706885285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>z</td>\n",
       "      <td>0.33548114554324376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>k</td>\n",
       "      <td>0.29229979935629863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>d</td>\n",
       "      <td>0.28581408720606105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>e</td>\n",
       "      <td>0.22142737666058276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TF-IDF\n",
       "7   0.5914293898242222\n",
       "5   0.5670138706885285\n",
       "z  0.33548114554324376\n",
       "k  0.29229979935629863\n",
       "d  0.28581408720606105\n",
       "e  0.22142737666058276\n",
       "t                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0\n",
       "                  0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(first_document_vector.T.todense(),index=vectorizer.get_feature_names(),columns=['TF-IDF'])\n",
    "df.sort_values(by=['TF-IDF'],ascending=False).astype(str).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split data into train & test\n",
    "    train---> To learn the relationship within data, \n",
    "    test-->  To do predictions, and this testing data will be unseen to my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37186522\n",
      "Iteration 2, loss = 0.26792794\n",
      "Iteration 3, loss = 0.19535225\n",
      "Iteration 4, loss = 0.15078579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier((300,),activation='relu',verbose=1,\n",
    "                  solver='adam',batch_size=32,learning_rate='constant',learning_rate_init=0.001,max_iter=20)\n",
    "mlp=mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_mlp = mlp.predict(X_test)\n",
    "y_train_mlp = mlp.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_mlp = accuracy_score(y_train,y_train_mlp)\n",
    "acc_test_mlp = accuracy_score(y_test,y_test_mlp)\n",
    "\n",
    "print(\"MLP: Accuracy on training Data: {:.3f}\".format(acc_train_mlp))\n",
    "print(\"MLP: Accuracy on test Data: {:.3f}\".format(acc_test_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import *\n",
    "visualize=ClassificationReport(mlp)\n",
    "visualize.fit(X_train,y_train)\n",
    "visualize.score(X_test,y_test)\n",
    "visualize.show(clear_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_graph_MLP=ROCAUC(mlp,classes=[0,1,2])\n",
    "roc_graph_MLP.fit(X_train,y_train)\n",
    "roc_graph_MLP.score(X_test,y_test)\n",
    "roc_graph_MLP.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE SCALING (REQUIRED FOR ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc= StandardScaler(with_mean=False)\n",
    "X_train_ann=sc.fit_transform(X_train)\n",
    "X_test_ann=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "ann=tf.keras.models.Sequential()\n",
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HIDDEN LAYERS WITH 6 NEURONS EACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, input_dim=input_dim,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6,activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=3,activation=tf.keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMPILE AND RUN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received uninstantiated Loss class: <class 'tensorflow.python.keras.losses.SparseCategoricalCrossentropy'>\nPlease call loss \"\"classes before passing them to Model.compile.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-0743cc33fa13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;31m# Prepare list of loss functions, same size of model outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m     self.loss_functions = training_utils.prepare_loss_functions(\n\u001b[1;32m--> 409\u001b[1;33m         self.loss, self.output_names)\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[0mtarget_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_target_tensor_for_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mprepare_loss_functions\u001b[1;34m(loss, output_names)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1454\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1455\u001b[1;33m     \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1457\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1454\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1455\u001b[1;33m     \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1457\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mget_loss_function\u001b[1;34m(loss)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     raise ValueError(\n\u001b[0;32m   1168\u001b[0m         \u001b[1;34m'Received uninstantiated Loss class: {}\\nPlease call loss \"\"classes '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m         'before passing them to Model.compile.'.format(loss))\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m   \u001b[1;31m# Deserialize loss configuration, if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Received uninstantiated Loss class: <class 'tensorflow.python.keras.losses.SparseCategoricalCrossentropy'>\nPlease call loss \"\"classes before passing them to Model.compile."
     ]
    }
   ],
   "source": [
    "ann.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy,metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A SciPy sparse matrix was passed to a model that expects dense inputs. Please densify your inputs first, such as by calling `x.toarray().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-0b35be9df2a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2426\u001b[0m       \u001b[0mconverted_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2427\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_expected_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2428\u001b[1;33m         \u001b[0mconverted_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_scipy_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2429\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverted_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_convert_scipy_sparse_tensor\u001b[1;34m(value, expected_input)\u001b[0m\n\u001b[0;32m   3196\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3197\u001b[0m         \u001b[1;31m# In TF2 we do not silently densify sparse matrices.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3198\u001b[1;33m         raise ValueError('A SciPy sparse matrix was passed to a model '\n\u001b[0m\u001b[0;32m   3199\u001b[0m                          \u001b[1;34m'that expects dense inputs. Please densify your '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3200\u001b[0m                          'inputs first, such as by calling `x.toarray().')\n",
      "\u001b[1;31mValueError\u001b[0m: A SciPy sparse matrix was passed to a model that expects dense inputs. Please densify your inputs first, such as by calling `x.toarray()."
     ]
    }
   ],
   "source": [
    "ann.fit(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Logistic on data as use-cas is Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(random_state=0,multi_class='multinomial',n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doing prediction for specific custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=np.array(['%@123abcd'])\n",
    "pred=vectorizer.transform(dt)\n",
    "clf.predict(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doing prediction on X-Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check Accuracy of your model using confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "print(\"The accuracy is =\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from yellowbrick.classifier import *\n",
    "visualize=ClassificationReport(clf)\n",
    "visualize.fit(X_train,y_train)\n",
    "visualize.score(X_test,y_test)\n",
    "visualize.show(clear_figure=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(max_depth=5,random_state=2)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred=forest.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate the model\n",
    "\n",
    "# fit the model \n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#predicting the target value from the model for the samples\n",
    "y_test_forest = forest.predict(X_test)\n",
    "y_train_forest = forest.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_forest = accuracy_score(y_train,y_train_forest)\n",
    "acc_test_forest = accuracy_score(y_test,y_test_forest)\n",
    "\n",
    "print(\"Random forest: Accuracy on training Data: {:.3f}\".format(acc_train_forest))\n",
    "print(\"Random forest: Accuracy on test Data: {:.3f}\".format(acc_test_forest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import *\n",
    "visualize=ClassificationReport(forest)\n",
    "visualize.fit(X_train,y_train)\n",
    "visualize.score(X_test,y_test)\n",
    "visualize.show(clear_figure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# instantiate the model \n",
    "tree = DecisionTreeClassifier(max_depth = 5)\n",
    "# fit the model \n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# instantiate the model \n",
    "tree = DecisionTreeClassifier(max_depth = 5)\n",
    "# fit the model \n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tree = tree.predict(X_test)\n",
    "y_train_tree = tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the accuracy of the model performance\n",
    "acc_train_tree = accuracy_score(y_train,y_train_tree)\n",
    "acc_test_tree = accuracy_score(y_test,y_test_tree)\n",
    "\n",
    "print(\"Decision Tree: Accuracy on training Data: {:.3f}\".format(acc_train_tree))\n",
    "print(\"Decision Tree: Accuracy on test Data: {:.3f}\".format(acc_test_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision recall f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import *\n",
    "visualize=ClassificationReport(tree)\n",
    "visualize.fit(X_train,y_train)\n",
    "visualize.score(X_test,y_test)\n",
    "visualize.show(clear_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# instantiate the model\n",
    "xgb = XGBClassifier(learning_rate=0.4,max_depth=7)\n",
    "#fit the model\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#predicting the target value from the model for the samples\n",
    "y_test_xgb = xgb.predict(X_test)\n",
    "y_train_xgb = xgb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the accuracy of the model performance\n",
    "acc_train_xgb = accuracy_score(y_train,y_train_xgb)\n",
    "acc_test_xgb = accuracy_score(y_test,y_test_xgb)\n",
    "\n",
    "print(\"XGBoost: Accuracy on training Data: {:.3f}\".format(acc_train_xgb))\n",
    "print(\"XGBoost : Accuracy on test Data: {:.3f}\".format(acc_test_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision recall f1 measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import *\n",
    "visualize=ClassificationReport(xgb)\n",
    "visualize.fit(X_train,y_train)\n",
    "visualize.score(X_test,y_test)\n",
    "visualize.show(clear_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC CURVE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    plot_roc_curve,\n",
    ")\n",
    "from yellowbrick.classifier import ROCAUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roc graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_graph_linear=ROCAUC(clf,classes=[0,1,2])\n",
    "roc_graph_linear.fit(X_train,y_train)\n",
    "roc_graph_linear.score(X_test,y_test)\n",
    "roc_graph_linear.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_graph_dt=ROCAUC(tree,classes=[0,1,2])\n",
    "roc_graph_dt.fit(X_train,y_train)\n",
    "roc_graph_dt.score(X_test,y_test)\n",
    "roc_graph_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_graph_xgb=ROCAUC(xgb,classes=[0,1,2])\n",
    "roc_graph_xgb.fit(X_train,y_train)\n",
    "roc_graph_xgb.score(X_test,y_test)\n",
    "roc_graph_xgb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"autoencoder.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "#Training the model\n",
    "history = autoencoder.fit(X_train, X_train, epochs=10, batch_size=64, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
